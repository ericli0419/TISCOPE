{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b4c2c5-1952-4da6-833a-acf1fc30264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "import scipy as sci\n",
    "import squidpy as sq\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bdb091-9393-496b-b2ec-66f35275cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['axes.unicode_minus']=False\n",
    "plt.rc('font', family='Helvetica')\n",
    "plt.rcParams['pdf.fonttype'] = 42\n",
    "sc.settings.verbosity = 3             # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.logging.print_header()\n",
    "sc.set_figure_params(dpi=120,facecolor='w',frameon=True,figsize=(4,4)) \n",
    "%config InlineBackend.figure_format='retina'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcb728d-6218-4a39-8de3-d8290318c62b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c35b1-4591-4a9d-b52d-8ac03a66cf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=sc.read_h5ad('/data2/liyuzhe/data/SPACEX/adata_ADmouse.h5ad')\n",
    "adata=adata[adata.obs.label.isin(['8months-control-replicate_1', '8months-control-replicate_2', \n",
    "                                  '13months-control-replicate_1', '13months-control-replicate_2'])]\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e9f36-851e-4989-bb2e-8cca1216c990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from function import TISCOPE_integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e930c30-ace1-4bb3-8183-89171ff16bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=TISCOPE_integration(adata,GPU=2,outdir='AD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e58a2b9-261d-43fb-8136-494e02c46d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata,color=['batch','tissue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b43c3-e401-4be4-bb4b-ba48a9c1b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.louvain(adata,resolution=0.02,neighbors_key='SPACE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5c69f-c318-4928-aa13-4b5663d521ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.cluster import adjusted_rand_score as ARI\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score as NMI\n",
    "from sklearn.metrics import homogeneity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124492ae-3b63-4201-90e0-acb7e31db1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI(adata.obs['tissue'].values,adata.obs['louvain'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55663391-b2cb-4ca8-9723-df8c9951eddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMI(adata.obs['tissue'].values,adata.obs['louvain'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab2fdd-dc23-487d-893b-af29cac559d5",
   "metadata": {},
   "source": [
    "# Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d4c4e-7330-44a8-8fcd-a45e855f3310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from function import TISCOPE_projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf74ed0b-21b4-4b52-8f52-087a0d32b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_ref=sc.read('AD/adata.h5ad')\n",
    "adata=sc.read('/data2/liyuzhe/data/SPACEX/adata_ADmouse.h5ad')\n",
    "adata=adata[adata.obs.label.isin(['8months-disease-replicate_1', '8months-disease-replicate_2',\n",
    "                                  '13months-disease-replicate_1', '13months-disease-replicate_2'])]\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a74450-9873-4a2b-be9e-cc8cb01de03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=TISCOPE_projection(adata,adata_ref,outdir='./AD',model_path='./AD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee54c65b-7811-498b-ac41-dd82b926bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.set_figure_params(dpi=120,facecolor='w',frameon=True,figsize=(4,4.5)) \n",
    "sc.pl.umap(adata,color=['tissue','projection'],ncols=1,legend_fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111910b3-62bb-4d3c-98ac-ab3627c4cfb9",
   "metadata": {},
   "source": [
    "# Label transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515b1f68-ab3a-442b-82f2-42006166d02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata=sc.read('./AD/adata_projection.h5ad')\n",
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1dbb9-d0b8-4124-966a-edc3ddc60744",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_query=adata[adata.obs.projection=='query']\n",
    "adata_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5362468-fa0e-4aa3-bcc2-479944b7ce8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_ref=adata[adata.obs.projection=='reference']\n",
    "adata_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1fcbd3-fe95-4ff8-bee2-f2c383704337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Fit kNN classifier on the reference\n",
    "knn = KNeighborsClassifier(n_neighbors=20, weights='distance')\n",
    "knn.fit(X=adata_ref.obsm['latent'], y=adata_ref.obs[['tissue']])\n",
    "\n",
    "# Predict labels with the knn classifier\n",
    "proba = knn.predict_proba(adata_query.obsm['latent'])\n",
    "k_dist, k_indx = knn.kneighbors(adata_query.obsm['latent'], n_neighbors=20, return_distance=True)\n",
    "\n",
    "predictions = proba\n",
    "predictions = pd.DataFrame({'tissue_transfer': np.argmax(predictions, axis=1), 'probability': np.max(predictions, axis=1), \n",
    "                            'mean_dist': np.mean(k_dist, axis=1), 'k_dist': k_dist[:,19]})\n",
    "predictions['tissue_transfer'] = predictions['tissue_transfer'].map({i: l for i, l in enumerate(knn.classes_)})\n",
    "predictions.index = adata_query.obs.index\n",
    "\n",
    "adata_query.obs = pd.concat([adata_query.obs, predictions], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3bacef-eec9-4fd3-9367-01bea3722186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca551f12-b11c-4f54-ae9b-9fa0d92c6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(adata_query.obs['tissue'], adata_query.obs['tissue_transfer'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e88b1-0367-4017-af88-7868e73831db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARI(adata_query.obs['tissue'], adata_query.obs['tissue_transfer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596417bc-6a9c-41e2-b959-3d9cf0a04f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "NMI(adata_query.obs['tissue'], adata_query.obs['tissue_transfer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79102bb-c5a7-4472-a9ef-0bca50f53f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=adata.obs.tissue.unique()\n",
    "confusion = confusion_matrix(adata_query.obs['tissue'],adata_query.obs['tissue_transfer'],labels=classes)\n",
    "row_sums = confusion.sum(axis=1)\n",
    "new_matrix = confusion / row_sums[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e85c94-6729-47c4-9c01-f5ba6d79502f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "\n",
    "# place labels at the top\n",
    "plt.gca().xaxis.tick_top()\n",
    "plt.gca().xaxis.set_label_position('top')\n",
    "\n",
    "# plot the matrix per se\n",
    "plt.imshow(new_matrix, interpolation='nearest', cmap=plt.cm.Blues,vmax=1)\n",
    "\n",
    "\n",
    "# plot colorbar to the right\n",
    "plt.colorbar()\n",
    "\n",
    "fmt = '.2f'\n",
    "\n",
    "# write the number of predictions in each bucket\n",
    "thresh = 0.2\n",
    "for i, j in itertools.product(range(confusion.shape[0]), range(confusion.shape[1])):\n",
    "\n",
    "    # # if background is dark, use a white number, and vice-versa\n",
    "    # plt.text(j, i,format(new_matrix[i, j], fmt),\n",
    "    #          horizontalalignment=\"center\",\n",
    "    #          fontsize=10,\n",
    "    #          color=\"white\" if new_matrix[i, j] > thresh else \"black\")\n",
    "    if new_matrix[i, j] > thresh:\n",
    "        plt.text(j, i,format(new_matrix[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 fontsize=10,\n",
    "                 color=\"white\")\n",
    "\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=90,fontsize=12)\n",
    "plt.yticks(tick_marks, classes, fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label',size=10)\n",
    "plt.xlabel('Predicted label',size=10)\n",
    "# plt.savefig('figures/AD_projection_confusion_matrix.pdf',bbox_inches='tight')\n",
    "# plt.savefig('figures/AD_projection_confusion_matrix.png',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33609e18-5a39-4473-9b98-285e4f0803e2",
   "metadata": {},
   "source": [
    "# Condition associated TM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378c6b3-eb24-41b0-b3a7-236d286d2c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "def calculate_tm_enrichment(adata, \n",
    "                           sample_type_col='Sample_type', \n",
    "                           batch_col='batch', \n",
    "                           cc_col='TM'):\n",
    "    \"\"\"\n",
    "    计算每个样本类型中各类 TM（如细胞类型）的富集得分。\n",
    "    \n",
    "    参数:\n",
    "        adata: AnnData 对象，包含 obs 表格。\n",
    "        sample_type_col: str, 样本类型的列名，默认 'Sample_type'\n",
    "        batch_col: str, 批次信息的列名，默认 'batch'\n",
    "        cc_col: str, 细胞类别（TM）的列名，默认 'cc'\n",
    "\n",
    "    返回:\n",
    "        dict: 每个 Sample_type 对应一个 DataFrame，包含各 TM 的 enrichment scores、AUC、p 值等。\n",
    "    \"\"\"\n",
    "    # 从 adata.obs 中提取所需列\n",
    "    obs_df = adata.obs[[sample_type_col, batch_col, cc_col]].copy()\n",
    "    obs_df.columns = ['Sample_type', 'batch', 'cc']  # 内部统一命名以简化后续处理\n",
    "\n",
    "    # 计算每个样本中每个 TM 的细胞计数\n",
    "    count_df = obs_df.groupby(['batch', 'cc']).size().unstack(fill_value=0)\n",
    "\n",
    "    # 计算每个样本的总细胞数\n",
    "    sample_totals = count_df.sum(axis=1)\n",
    "\n",
    "    # 计算比例矩阵\n",
    "    prop_df = count_df.div(sample_totals, axis=0)\n",
    "\n",
    "    # 将 condition（Sample_type）信息添加到比例矩阵中\n",
    "    sample_conditions = obs_df[['batch', 'Sample_type']].drop_duplicates().set_index('batch')['Sample_type']\n",
    "    prop_df['Sample_type'] = sample_conditions\n",
    "\n",
    "    # 初始化结果字典\n",
    "    results_dict = {}\n",
    "\n",
    "    # 获取所有 conditions 和所有 TMs\n",
    "    conditions = prop_df['Sample_type'].unique()\n",
    "    tms = count_df.columns\n",
    "\n",
    "    # 遍历每个 condition\n",
    "    for c in conditions:\n",
    "        scores = []\n",
    "        auc_values = []\n",
    "        p_values = []\n",
    "        tm_names = []\n",
    "\n",
    "        # 遍历每个 TM\n",
    "        for t in tms:\n",
    "            # 提取当前 condition 下该 TM 的比例\n",
    "            prop_c = prop_df[prop_df['Sample_type'] == c][t]\n",
    "            # 提取其他 condition 下该 TM 的比例\n",
    "            prop_other = prop_df[prop_df['Sample_type'] != c][t]\n",
    "\n",
    "            # 数据检查\n",
    "            if len(prop_c) == 0 or len(prop_other) == 0:\n",
    "                auc = 0.5\n",
    "                p_value = 1.0\n",
    "            else:\n",
    "                try:\n",
    "                    # Mann-Whitney U 检验\n",
    "                    u_stat, p_value = mannwhitneyu(prop_c, prop_other, alternative='two-sided')\n",
    "                    n1 = len(prop_c)\n",
    "                    n2 = len(prop_other)\n",
    "                    auc = u_stat / (n1 * n2)  # AUC 近似计算\n",
    "                except Exception:\n",
    "                    auc = 0.5\n",
    "                    p_value = 1.0\n",
    "\n",
    "            # 计算 enrichment score\n",
    "            score = auc - 0.5\n",
    "\n",
    "            # 存储结果\n",
    "            tm_names.append(t)\n",
    "            scores.append(score)\n",
    "            auc_values.append(auc)\n",
    "            p_values.append(p_value)\n",
    "\n",
    "        # 多重检验校正（FDR）\n",
    "        rejected, p_adjusted, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "        # 构建结果 DataFrame\n",
    "        result_df = pd.DataFrame({\n",
    "            'TM': tm_names,\n",
    "            'score': scores,\n",
    "            'AUC': auc_values,\n",
    "            'p_value': p_values,\n",
    "            'p_adjusted': p_adjusted\n",
    "        })\n",
    "\n",
    "        # 按得分降序排序并重置索引\n",
    "        result_df = result_df.sort_values('score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "        # 添加到结果字典\n",
    "        results_dict[c] = result_df\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea65e98d-39f7-4a8c-8ae3-2252e18898a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_tm_enrichment(adata,sample_type_col='group', \n",
    "                           batch_col='batch', \n",
    "                           cc_col='region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad1edc-838d-43bc-a3d8-172f849d1976",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tm_enrichment_bubble(results_dict, top_n=15, condition_order=None, tm_order=None, \n",
    "                                   sig_threshold=0.05):\n",
    "    \"\"\"\n",
    "    使用气泡图可视化TM富集分析结果，并为显著点添加红圈\n",
    "    \n",
    "    参数:\n",
    "    results_dict: 从calculate_tm_enrichment函数返回的结果字典，必须包含 'p_adjusted' 和 'score'\n",
    "    top_n: 每个条件显示前N个和后N个TM\n",
    "    condition_order: list, 条件的显示顺序，如 ['Control', 'Disease']\n",
    "    tm_order: list, TM的显示顺序（可选）\n",
    "    sig_threshold: float, 显著性阈值，默认0.05\n",
    "    \"\"\"\n",
    "\n",
    "    # 合并所有结果\n",
    "    all_results = []\n",
    "    for condition, df in results_dict.items():\n",
    "        df = df.copy()\n",
    "        df['condition'] = condition\n",
    "        all_results.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_results)\n",
    "\n",
    "    # 确保包含必要的列\n",
    "    required_columns = {'score', 'p_adjusted', 'TM'}\n",
    "    for df in results_dict.values():\n",
    "        if not required_columns.issubset(df.columns):\n",
    "            raise ValueError(f\"Missing required columns: {required_columns - set(df.columns)}\")\n",
    "\n",
    "    # 为每个条件选择前N个和后N个TM\n",
    "    top_bottom_dfs = []\n",
    "    for condition in combined_df['condition'].unique():\n",
    "        cond_df = combined_df[combined_df['condition'] == condition].copy()\n",
    "        cond_df = cond_df.sort_values('score', ascending=False)\n",
    "        top_df = cond_df.head(top_n)\n",
    "        bottom_df = cond_df.tail(top_n)\n",
    "        top_bottom_dfs.extend([top_df, bottom_df])\n",
    "    \n",
    "    selected_df = pd.concat(top_bottom_dfs)\n",
    "\n",
    "    # 获取所有需要显示的conditions和TMs\n",
    "    if condition_order is not None:\n",
    "        display_conditions = [c for c in condition_order if c in selected_df['condition'].unique()]\n",
    "    else:\n",
    "        display_conditions = sorted(selected_df['condition'].unique())\n",
    "    \n",
    "    if tm_order is not None:\n",
    "        display_tms = [tm for tm in tm_order if tm in selected_df['TM'].unique()]\n",
    "    else:\n",
    "        display_tms = list(selected_df['TM'].unique())\n",
    "\n",
    "    # 创建完整的condition-TM组合网格\n",
    "    full_combinations = []\n",
    "    for condition in display_conditions:\n",
    "        for tm in display_tms:\n",
    "            full_combinations.append({'condition': condition, 'TM': tm})\n",
    "    \n",
    "    full_grid_df = pd.DataFrame(full_combinations)\n",
    "\n",
    "    # 首先从selected_df中查找这些组合的数据\n",
    "    plot_df = full_grid_df.merge(selected_df[['condition', 'TM', 'score', 'p_adjusted']], \n",
    "                                on=['condition', 'TM'], \n",
    "                                how='left')\n",
    "\n",
    "    # 如果仍然缺失，则从原始数据中查找\n",
    "    missing_mask = plot_df['score'].isna()\n",
    "    if missing_mask.any():\n",
    "        missing_combinations = plot_df[missing_mask][['condition', 'TM']]\n",
    "        found_data = missing_combinations.merge(\n",
    "            combined_df[['condition', 'TM', 'score', 'p_adjusted']], \n",
    "            on=['condition', 'TM'], \n",
    "            how='left'\n",
    "        )\n",
    "        plot_df.loc[missing_mask, 'score'] = found_data['score'].values\n",
    "        plot_df.loc[missing_mask, 'p_adjusted'] = found_data['p_adjusted'].values\n",
    "\n",
    "    # 添加是否显著的列\n",
    "    plot_df['significant'] = plot_df['p_adjusted'] < sig_threshold\n",
    "\n",
    "    # 设置坐标映射\n",
    "    cond_to_num = {cond: i for i, cond in enumerate(display_conditions)}\n",
    "    tm_to_num = {tm: i for i, tm in enumerate(display_tms)}\n",
    "    plot_df['x_pos'] = plot_df['condition'].map(cond_to_num)\n",
    "    plot_df['y_pos'] = plot_df['TM'].map(tm_to_num)\n",
    "\n",
    "    # 开始绘图\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # 主要气泡图\n",
    "    scatter = plt.scatter(\n",
    "        x=plot_df['x_pos'],\n",
    "        y=plot_df['y_pos'],\n",
    "        s=np.abs(plot_df['score']) * 1000 + 50,\n",
    "        c=plot_df['score'],\n",
    "        cmap='RdBu_r',\n",
    "        alpha=0.7,\n",
    "        edgecolors='black',\n",
    "        linewidth=0.5,\n",
    "        vmin=-0.5,\n",
    "        vmax=0.5\n",
    "    )\n",
    "\n",
    "    # 绘制显著点的红圈\n",
    "    sig_points = plot_df[plot_df['significant']]\n",
    "    if not sig_points.empty:\n",
    "        plt.scatter(\n",
    "            x=sig_points['x_pos'],\n",
    "            y=sig_points['y_pos'],\n",
    "            s=(np.abs(sig_points['score']) * 1000 + 50),  # 与主图一致大小\n",
    "            facecolors='none',\n",
    "            edgecolors='red',\n",
    "            linewidth=2,\n",
    "            label=' Significant (p < {})'.format(sig_threshold)\n",
    "        )\n",
    "\n",
    "    # 添加颜色条\n",
    "    cbar = plt.colorbar(scatter)\n",
    "    cbar.set_label('Enrichment Score', fontsize=12)\n",
    "\n",
    "    # 设置坐标轴标签\n",
    "    plt.xticks(ticks=list(cond_to_num.values()), labels=list(cond_to_num.keys()), rotation=45, ha='right')\n",
    "    plt.yticks(ticks=list(tm_to_num.values()), labels=list(tm_to_num.keys()))\n",
    "    plt.xlabel('Condition', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('TM', fontsize=12, fontweight='bold')\n",
    "    plt.xlim(-0.5, len(display_conditions) - 0.5)\n",
    "    \n",
    "\n",
    "    # 图例\n",
    "    # plt.legend(loc='upper right')\n",
    "    plt.legend(loc=(1.2,0.8), frameon=False)\n",
    "\n",
    "    # 布局调整\n",
    "    plt.tight_layout()\n",
    "    plt.grid(False)\n",
    "    # plt.savefig('figures/dotplot_colitis_TM_enrich_score.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901fb758-7b2e-4fc9-a0dd-b32be69e0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tm_enrichment_bubble(results, top_n=100,sig_threshold=0.05,\n",
    "                               condition_order=['control', 'disease'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd382f6-0e34-4331-9008-37840f7c1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import sparse\n",
    "import warnings\n",
    "\n",
    "def overcorrection_score(emb, celltype, n_neighbors=100, n_samples=None, \n",
    "                                 min_cells_per_type=5, random_state=42, weighted=False):\n",
    "    \"\"\"\n",
    "    Improved overcorrection score that evaluates whether integration has over-mixed\n",
    "    biological cell types while removing batch effects.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    emb : array-like, shape (n_cells, n_features)\n",
    "        Embedding coordinates (e.g., UMAP, PCA)\n",
    "    celltype : array-like, shape (n_cells,)\n",
    "        Cell type labels for each cell\n",
    "    n_neighbors : int, optional (default=30)\n",
    "        Number of neighbors to consider for each cell\n",
    "    n_samples : int, optional (default=1000)\n",
    "        Number of cells to sample for estimation (use None for all cells)\n",
    "    min_cells_per_type : int, optional (default=5)\n",
    "        Minimum number of cells required for a cell type to be included in scoring\n",
    "    random_state : int, optional (default=42)\n",
    "        Random seed for reproducibility\n",
    "    weighted : bool, optional (default=True)\n",
    "        Whether to weight scores by cell type prevalence\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    score : float\n",
    "        Overcorrection score (higher values indicate more overcorrection)\n",
    "    celltype_scores : dict\n",
    "        Dictionary with scores for each cell type\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if len(emb) != len(celltype):\n",
    "        raise ValueError(\"emb and celltype must have the same length\")\n",
    "    \n",
    "    if n_neighbors >= len(emb):\n",
    "        warnings.warn(f\"n_neighbors ({n_neighbors}) is too large for dataset size ({len(emb)}). \"\n",
    "                      f\"Reducing to {len(emb) - 1}\")\n",
    "        n_neighbors = len(emb) - 1\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    emb = np.asarray(emb)\n",
    "    celltype = np.asarray(celltype)\n",
    "    \n",
    "    # Get unique cell types and their counts\n",
    "    unique_types, type_counts = np.unique(celltype, return_counts=True)\n",
    "    \n",
    "    # Filter out cell types with too few cells\n",
    "    valid_types = unique_types[type_counts >= min_cells_per_type]\n",
    "    if len(valid_types) < 2:\n",
    "        raise ValueError(f\"Need at least 2 cell types with ≥ {min_cells_per_type} cells each\")\n",
    "    \n",
    "    # Create a mask for valid cells\n",
    "    valid_mask = np.isin(celltype, valid_types)\n",
    "    \n",
    "    if not np.all(valid_mask):\n",
    "        warnings.warn(f\"Ignoring {np.sum(~valid_mask)} cells from rare cell types \"\n",
    "                      f\"(< {min_cells_per_type} cells)\")\n",
    "    \n",
    "    # Subset to valid cells\n",
    "    emb_valid = emb[valid_mask]\n",
    "    celltype_valid = celltype[valid_mask]\n",
    "    \n",
    "    # Build nearest neighbors graph\n",
    "    nne = NearestNeighbors(n_neighbors=min(n_neighbors + 1, len(emb_valid)), \n",
    "                           n_jobs=-1)  # Use all available cores\n",
    "    nne.fit(emb_valid)\n",
    "    kmatrix = nne.kneighbors_graph(emb_valid, mode='connectivity')\n",
    "    \n",
    "    # Remove self-connections\n",
    "    kmatrix = kmatrix - sparse.identity(kmatrix.shape[0])\n",
    "    \n",
    "    # Sample cells if requested\n",
    "    if n_samples is not None and n_samples < len(emb_valid):\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        sample_indices = rng.choice(len(emb_valid), size=n_samples, replace=False)\n",
    "    else:\n",
    "        sample_indices = np.arange(len(emb_valid))\n",
    "    \n",
    "    # Calculate scores per cell type\n",
    "    celltype_scores = {}\n",
    "    for ct in valid_types:\n",
    "        # Get indices of cells of this type\n",
    "        ct_indices = np.where(celltype_valid == ct)[0]\n",
    "        \n",
    "        # Sample from this cell type if needed\n",
    "        if n_samples is not None:\n",
    "            n_sample_ct = max(1, int(n_samples * len(ct_indices) / len(emb_valid)))\n",
    "            if n_sample_ct < len(ct_indices):\n",
    "                ct_sample_indices = rng.choice(ct_indices, size=n_sample_ct, replace=False)\n",
    "            else:\n",
    "                ct_sample_indices = ct_indices\n",
    "        else:\n",
    "            ct_sample_indices = ct_indices\n",
    "        \n",
    "        # Calculate average same-type proportion for this cell type\n",
    "        same_type_props = []\n",
    "        for i in ct_sample_indices:\n",
    "            # Get neighbors (excluding self)\n",
    "            neighbors = kmatrix[i].nonzero()[1]\n",
    "            \n",
    "            # Calculate proportion of same-type neighbors\n",
    "            same_type_count = np.sum(celltype_valid[neighbors] == celltype_valid[i])\n",
    "            same_type_prop = same_type_count / len(neighbors) if len(neighbors) > 0 else 0\n",
    "            same_type_props.append(same_type_prop)\n",
    "        \n",
    "        celltype_scores[ct] = np.mean(same_type_props) if same_type_props else 0\n",
    "    \n",
    "    # Calculate overall score\n",
    "    if weighted:\n",
    "        # Weight by cell type prevalence\n",
    "        weights = [type_counts[unique_types == ct][0] for ct in valid_types]\n",
    "        overall_score = 1 - np.average(list(celltype_scores.values()), weights=weights)\n",
    "    else:\n",
    "        # Simple average across cell types\n",
    "        overall_score = 1 - np.mean(list(celltype_scores.values()))\n",
    "    \n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45464fd-111f-4fce-9a28-7d349ab713a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "overcorrection_score(adata.obsm['X_umap'], adata.obs['tissue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297cba8-e841-4669-98ab-6a8ea65313ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
